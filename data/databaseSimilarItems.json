[
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ],
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ],
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ],
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ],
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ],
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ],
    [
        "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations",
        false
    ],
    [
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development",
        "Standards for AI Governance: International Standards to Enable Global Coordination in AI Research &amp; Development",
        true
    ],
    [
        "Why those who care about catastrophic and existential risk should care about autonomous weapons - LessWrong",
        "Why those who care about catastrophic and existential risk should care about autonomous weapons",
        true
    ],
    [
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.",
        "Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter",
        true
    ],
    [
        "Artificial Intelligence: American Attitudes and Trends",
        "FLI Podcast- Artificial Intelligence: American Attitudes and Trends",
        false
    ],
    [
        "Embedded Agency",
        "Embedded Agents",
        false
    ],
    [
        "How Much Computational Power Does It Take to Match the Human Brain?",
        "How Much Computational Power It Takes to Match the Human Brain",
        true
    ],
    [
        "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        "2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",
        false
    ],
    [
        "Learning Human Objectives by Evaluating Hypothetical Behavior",
        "Learning human objectives by evaluating hypothetical behaviours",
        true
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2019 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2020 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "2016 AI Risk Literature Review and Charity Comparison",
        "2018 AI Alignment Literature Review and Charity Comparison",
        false
    ],
    [
        "Tradeoff between desirable properties for baseline choices in impact measures",
        "Tradeoffs between desirable properties for baseline choices in impact measures",
        true
    ],
    [
        "Coherence arguments do not imply goal-directed behavior",
        "Comment on Coherence arguments do not imply goal directed behavior",
        false
    ],
    [
        "Classification of global catastrophic risks connected with artificial intelligence",
        "Classiﬁcation of global catastrophic risks connected with artiﬁcial intelligence",
        true
    ],
    [
        "Multi-task Maximum Entropy Inverse Reinforcement Learning",
        "Maximum Entropy Inverse Reinforcement Learning",
        false
    ],
    [
        "AI Research Considerations for Human Existential Safety (ARCHES)",
        "AI Research Considerations for Human Existential Safety",
        true
    ],
    [
        "Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",
        "Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda",
        true
    ]
]